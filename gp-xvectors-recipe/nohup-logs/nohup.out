Obtaining renewable credentials, you will be asked to type in your password
Password for s1513472@INF.ED.AC.UK: 
Running job "./run.sh --experiment=baseline --stage=4" for maximum of 28 days in background
Waiting for job to start...
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 	 This shell script runs the GlobalPhone+X-vectors recipe.
 	 Use like this: ./run.sh <options>
 	 --stage=INT		Stage from which to start
 	 --run-all=(false|true)	Whether to run all stages
 	 			or just the specified one
 	 --experiment=STR	Experiment name (also name of directory 
 	 			where all files will be stored).
 	 			Default: 'baseline'.

 	 If no stage number is provided, either all stages
 	 will be run (--run-all=true) or no stages at all.
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Running only stage 4.
Conda environment not activated, sourcing ~/.bashrc and activating the 'lid' env.
Running on the cluster.
Using shorten (v3.6.1) from ~/language-ident-from-speech/gp-xvectors-recipe/tools/shorten-3.6.1/bin/shorten
Using sox (v14.3.2) from ~/language-ident-from-speech/gp-xvectors-recipe/tools/sox-14.3.2/bin/sox
Running with languages: AR BG CH CR CZ FR GE JA KO PL PO RU SP SW TA TH TU WU VN
#### STAGE 4: Training the X-vector DNN. ####
Running on the cluster.
Running on the cluster.
/home/s1513472/lid/baseline/nnet_train_data
/home/s1513472/lid/baseline/nnet/egs
#### STAGE 4: Getting NN training egs. ####
./local/get_egs.sh --cmd slurm.pl --nj 32 --stage 0 --frames-per-iter 50000000 --frames-per-iter-diagnostic 100000 --min-frames-per-chunk 200 --max-frames-per-chunk 400 --num-diagnostic-archives 3 --num-repeats 35 /home/s1513472/lid/baseline/nnet_train_data /home/s1513472/lid/baseline/nnet/egs
feat-to-dim scp:/home/s1513472/lid/baseline/nnet_train_data/feats.scp - 
./local/get_egs.sh: Preparing train and validation lists
./local/get_egs.sh: Producing 70 archives for training
./local/get_egs.sh: Allocating training examples
Job started successfully
./local/get_egs.sh: Allocating training subset examples
./local/get_egs.sh: Allocating validation examples
./local/get_egs.sh: Generating training examples on disk
./local/get_egs.sh: Generating training subset examples on disk
./local/get_egs.sh: Generating validation examples on disk
./local/get_egs.sh: Shuffling order of archives on disk
./local/get_egs.sh: Finished preparing training examples
#### STAGE 5: Creating NN configs using the xconfig parser. ####
./local/run_xvector.sh: creating neural net configs using the xconfig parser
nnet3-init /home/s1513472/lid/baseline/nnet/configs/ref.config /home/s1513472/lid/baseline/nnet/configs/ref.raw 
LOG (nnet3-init[5.5.142~5-ff514]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to /home/s1513472/lid/baseline/nnet/configs/ref.raw
nnet3-info /home/s1513472/lid/baseline/nnet/configs/ref.raw 
nnet3-init /home/s1513472/lid/baseline/nnet/configs/ref.config /home/s1513472/lid/baseline/nnet/configs/ref.raw 
LOG (nnet3-init[5.5.142~5-ff514]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to /home/s1513472/lid/baseline/nnet/configs/ref.raw
nnet3-info /home/s1513472/lid/baseline/nnet/configs/ref.raw 
steps/nnet3/xconfig_to_configs.py --xconfig-file /home/s1513472/lid/baseline/nnet/configs/network.xconfig --config-dir /home/s1513472/lid/baseline/nnet/configs
#### STAGE 6: Training the network. ####
2019-01-16 19:50:05,031 [steps/nnet3/train_raw_dnn.py:35 - <module> - INFO ] Starting raw DNN trainer (train_raw_dnn.py)
2019-01-16 19:50:05,249 [steps/nnet3/train_raw_dnn.py:156 - process_args - WARNING ] You are running with one thread but you have not compiled
                   for CUDA.  You may be running a setup optimized for GPUs.
                   If you have GPUs and have nvcc installed, go to src/ and do
                   ./configure; make
2019-01-16 19:50:05,251 [steps/nnet3/train_raw_dnn.py:195 - train - INFO ] Arguments for the experiment
{'backstitch_training_interval': 1,
 'backstitch_training_scale': 0.0,
 'cleanup': True,
 'cmvn_opts': None,
 'combine_sum_to_one_penalty': 0.0,
 'command': 'slurm.pl',
 'compute_average_posteriors': False,
 'compute_per_dim_accuracy': False,
 'dir': '/home/s1513472/lid/baseline/nnet',
 'do_final_combination': True,
 'dropout_schedule': '0,0@0.20,0.1@0.50,0',
 'egs_command': None,
 'egs_dir': '/home/s1513472/lid/baseline/nnet/egs',
 'egs_opts': None,
 'egs_stage': 0,
 'email': None,
 'exit_stage': None,
 'feat_dir': None,
 'final_effective_lrate': 0.0001,
 'frames_per_eg': 1,
 'image_augmentation_opts': None,
 'initial_effective_lrate': 0.001,
 'input_model': None,
 'max_lda_jobs': 10,
 'max_models_combine': 20,
 'max_objective_evaluations': 30,
 'max_param_change': 2.0,
 'minibatch_size': '64',
 'momentum': 0.5,
 'nj': 4,
 'num_epochs': 3.0,
 'num_jobs_compute_prior': 10,
 'num_jobs_final': 3,
 'num_jobs_initial': 3,
 'online_ivector_dir': None,
 'preserve_model_interval': 10,
 'presoftmax_prior_scale_power': -0.25,
 'prior_subset_size': 20000,
 'proportional_shrink': 10.0,
 'rand_prune': 4.0,
 'remove_egs': False,
 'reporting_interval': 0.1,
 'samples_per_iter': 400000,
 'shuffle_buffer_size': 1000,
 'srand': 123,
 'stage': -1,
 'targets_scp': None,
 'train_opts': [],
 'use_dense_targets': True,
 'use_gpu': 'yes'}
2019-01-16 19:50:05,282 [steps/nnet3/train_raw_dnn.py:315 - train - INFO ] Preparing the initial network.
2019-01-16 19:50:13,758 [steps/nnet3/train_raw_dnn.py:353 - train - INFO ] Training will run for 3.0 epochs = 70 iterations
2019-01-16 19:50:13,759 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 0/69    Epoch: 0.00/3.0 (0.0% complete)    lr: 0.003000    shrink: 0.97000
2019-01-16 19:54:42,659 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 1/69    Epoch: 0.04/3.0 (1.4% complete)    lr: 0.002903    shrink: 0.97097
2019-01-16 20:01:21,761 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 2/69    Epoch: 0.09/3.0 (2.9% complete)    lr: 0.002809    shrink: 0.97191
2019-01-16 20:08:22,200 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 3/69    Epoch: 0.13/3.0 (4.3% complete)    lr: 0.002718    shrink: 0.97282
2019-01-16 20:15:53,853 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 4/69    Epoch: 0.17/3.0 (5.7% complete)    lr: 0.002630    shrink: 0.97370
2019-01-16 20:22:16,126 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 5/69    Epoch: 0.21/3.0 (7.1% complete)    lr: 0.002545    shrink: 0.97455
2019-01-16 20:30:14,443 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 6/69    Epoch: 0.26/3.0 (8.6% complete)    lr: 0.002463    shrink: 0.97537
2019-01-16 20:37:28,130 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 7/69    Epoch: 0.30/3.0 (10.0% complete)    lr: 0.002383    shrink: 0.97617
2019-01-16 20:43:10,172 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 8/69    Epoch: 0.34/3.0 (11.4% complete)    lr: 0.002306    shrink: 0.97694
2019-01-16 20:50:38,056 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 9/69    Epoch: 0.39/3.0 (12.9% complete)    lr: 0.002231    shrink: 0.97769
2019-01-16 20:58:45,281 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 10/69    Epoch: 0.43/3.0 (14.3% complete)    lr: 0.002159    shrink: 0.97841
2019-01-16 21:06:10,363 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 11/69    Epoch: 0.47/3.0 (15.7% complete)    lr: 0.002089    shrink: 0.97911
2019-01-16 21:14:21,128 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 12/69    Epoch: 0.51/3.0 (17.1% complete)    lr: 0.002022    shrink: 0.97978
2019-01-16 21:19:39,744 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 13/69    Epoch: 0.56/3.0 (18.6% complete)    lr: 0.001956    shrink: 0.98044
2019-01-16 21:27:36,413 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 14/69    Epoch: 0.60/3.0 (20.0% complete)    lr: 0.001893    shrink: 0.98107
2019-01-16 21:34:03,913 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 15/69    Epoch: 0.64/3.0 (21.4% complete)    lr: 0.001832    shrink: 0.98168
2019-01-16 21:39:15,766 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 16/69    Epoch: 0.69/3.0 (22.9% complete)    lr: 0.001772    shrink: 0.98228
2019-01-16 21:46:52,798 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 17/69    Epoch: 0.73/3.0 (24.3% complete)    lr: 0.001715    shrink: 0.98285
2019-01-16 21:52:29,089 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 18/69    Epoch: 0.77/3.0 (25.7% complete)    lr: 0.001660    shrink: 0.98340
2019-01-16 21:59:42,157 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 19/69    Epoch: 0.81/3.0 (27.1% complete)    lr: 0.001606    shrink: 0.98394
2019-01-16 22:07:19,330 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 20/69    Epoch: 0.86/3.0 (28.6% complete)    lr: 0.001554    shrink: 0.98446
2019-01-16 22:14:50,844 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 21/69    Epoch: 0.90/3.0 (30.0% complete)    lr: 0.001504    shrink: 0.98496
2019-01-16 22:22:31,263 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 22/69    Epoch: 0.94/3.0 (31.4% complete)    lr: 0.001455    shrink: 0.98545
2019-01-16 22:29:35,087 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 23/69    Epoch: 0.99/3.0 (32.9% complete)    lr: 0.001408    shrink: 0.98592
2019-01-16 22:34:19,734 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 24/69    Epoch: 1.03/3.0 (34.3% complete)    lr: 0.001362    shrink: 0.98638
2019-01-16 22:41:02,513 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 25/69    Epoch: 1.07/3.0 (35.7% complete)    lr: 0.001318    shrink: 0.98682
2019-01-16 22:48:07,679 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 26/69    Epoch: 1.11/3.0 (37.1% complete)    lr: 0.001276    shrink: 0.98724
2019-01-16 22:55:26,919 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 27/69    Epoch: 1.16/3.0 (38.6% complete)    lr: 0.001234    shrink: 0.98766
2019-01-16 23:02:59,586 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 28/69    Epoch: 1.20/3.0 (40.0% complete)    lr: 0.001194    shrink: 0.98806
2019-01-16 23:11:04,332 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 29/69    Epoch: 1.24/3.0 (41.4% complete)    lr: 0.001156    shrink: 0.98844
2019-01-16 23:18:21,612 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 30/69    Epoch: 1.29/3.0 (42.9% complete)    lr: 0.001118    shrink: 0.98882
2019-01-16 23:25:34,388 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 31/69    Epoch: 1.33/3.0 (44.3% complete)    lr: 0.001082    shrink: 0.98918
2019-01-16 23:33:02,995 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 32/69    Epoch: 1.37/3.0 (45.7% complete)    lr: 0.001047    shrink: 0.98953
2019-01-16 23:40:46,751 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 33/69    Epoch: 1.41/3.0 (47.1% complete)    lr: 0.001013    shrink: 0.98987
2019-01-16 23:48:51,593 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 34/69    Epoch: 1.46/3.0 (48.6% complete)    lr: 0.000980    shrink: 0.99020
2019-01-16 23:56:18,328 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 35/69    Epoch: 1.50/3.0 (50.0% complete)    lr: 0.000949    shrink: 0.99051
2019-01-17 00:04:29,312 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 36/69    Epoch: 1.54/3.0 (51.4% complete)    lr: 0.000918    shrink: 0.99082
2019-01-17 00:12:29,316 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 37/69    Epoch: 1.59/3.0 (52.9% complete)    lr: 0.000888    shrink: 0.99112
2019-01-17 00:18:54,426 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 38/69    Epoch: 1.63/3.0 (54.3% complete)    lr: 0.000860    shrink: 0.99140
2019-01-17 00:24:03,612 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 39/69    Epoch: 1.67/3.0 (55.7% complete)    lr: 0.000832    shrink: 0.99168
2019-01-17 00:31:32,038 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 40/69    Epoch: 1.71/3.0 (57.1% complete)    lr: 0.000805    shrink: 0.99195
2019-01-17 00:39:09,977 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 41/69    Epoch: 1.76/3.0 (58.6% complete)    lr: 0.000779    shrink: 0.99221
2019-01-17 00:46:23,754 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 42/69    Epoch: 1.80/3.0 (60.0% complete)    lr: 0.000754    shrink: 0.99246
2019-01-17 00:52:46,129 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 43/69    Epoch: 1.84/3.0 (61.4% complete)    lr: 0.000729    shrink: 0.99271
2019-01-17 01:00:27,976 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 44/69    Epoch: 1.89/3.0 (62.9% complete)    lr: 0.000706    shrink: 0.99294
2019-01-17 01:08:05,720 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 45/69    Epoch: 1.93/3.0 (64.3% complete)    lr: 0.000683    shrink: 0.99317
2019-01-17 01:13:55,797 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 46/69    Epoch: 1.97/3.0 (65.7% complete)    lr: 0.000661    shrink: 0.99339
2019-01-17 01:21:00,315 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 47/69    Epoch: 2.01/3.0 (67.1% complete)    lr: 0.000639    shrink: 0.99361
2019-01-17 01:26:44,209 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 48/69    Epoch: 2.06/3.0 (68.6% complete)    lr: 0.000619    shrink: 0.99381
2019-01-17 01:33:27,537 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 49/69    Epoch: 2.10/3.0 (70.0% complete)    lr: 0.000599    shrink: 0.99401
2019-01-17 01:40:47,716 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 50/69    Epoch: 2.14/3.0 (71.4% complete)    lr: 0.000579    shrink: 0.99421
2019-01-17 01:48:19,845 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 51/69    Epoch: 2.19/3.0 (72.9% complete)    lr: 0.000560    shrink: 0.99440
2019-01-17 01:54:42,171 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 52/69    Epoch: 2.23/3.0 (74.3% complete)    lr: 0.000542    shrink: 0.99458
2019-01-17 02:02:45,023 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 53/69    Epoch: 2.27/3.0 (75.7% complete)    lr: 0.000525    shrink: 0.99475
2019-01-17 02:10:02,940 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 54/69    Epoch: 2.31/3.0 (77.1% complete)    lr: 0.000508    shrink: 0.99492
2019-01-17 02:17:32,428 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 55/69    Epoch: 2.36/3.0 (78.6% complete)    lr: 0.000491    shrink: 0.99509
2019-01-17 02:25:19,993 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 56/69    Epoch: 2.40/3.0 (80.0% complete)    lr: 0.000475    shrink: 0.99525
2019-01-17 02:33:25,916 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 57/69    Epoch: 2.44/3.0 (81.4% complete)    lr: 0.000460    shrink: 0.99540
2019-01-17 02:40:49,450 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 58/69    Epoch: 2.49/3.0 (82.9% complete)    lr: 0.000445    shrink: 0.99555
2019-01-17 02:48:59,532 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 59/69    Epoch: 2.53/3.0 (84.3% complete)    lr: 0.000431    shrink: 0.99569
2019-01-17 02:54:19,621 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 60/69    Epoch: 2.57/3.0 (85.7% complete)    lr: 0.000417    shrink: 0.99583
2019-01-17 03:02:19,863 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 61/69    Epoch: 2.61/3.0 (87.1% complete)    lr: 0.000403    shrink: 0.99597
2019-01-17 03:08:48,835 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 62/69    Epoch: 2.66/3.0 (88.6% complete)    lr: 0.000390    shrink: 0.99610
2019-01-17 03:16:15,535 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 63/69    Epoch: 2.70/3.0 (90.0% complete)    lr: 0.000378    shrink: 0.99622
2019-01-17 03:23:51,255 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 64/69    Epoch: 2.74/3.0 (91.4% complete)    lr: 0.000365    shrink: 0.99635
2019-01-17 03:31:05,965 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 65/69    Epoch: 2.79/3.0 (92.9% complete)    lr: 0.000354    shrink: 0.99646
2019-01-17 03:36:32,227 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 66/69    Epoch: 2.83/3.0 (94.3% complete)    lr: 0.000342    shrink: 0.99658
2019-01-17 03:44:11,299 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 67/69    Epoch: 2.87/3.0 (95.7% complete)    lr: 0.000331    shrink: 0.99669
2019-01-17 03:51:50,548 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 68/69    Epoch: 2.91/3.0 (97.1% complete)    lr: 0.000320    shrink: 0.99680
2019-01-17 03:57:41,443 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 69/69    Epoch: 2.96/3.0 (98.6% complete)    lr: 0.000300    shrink: 0.99700
2019-01-17 04:04:44,240 [steps/nnet3/train_raw_dnn.py:440 - train - INFO ] Doing final combination to produce final.raw
2019-01-17 04:04:44,241 [steps/libs/nnet3/train/frame_level_objf/common.py:491 - combine_models - INFO ] Combining {51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70} models.
2019-01-17 04:08:55,763 [steps/nnet3/train_raw_dnn.py:462 - train - INFO ] Cleaning up the experiment directory /home/s1513472/lid/baseline/nnet
/home/s1513472/lid/baseline/nnet: num-iters=70 nj=3..3 num-params=4.5M dim=23->19 combine=-0.08->-0.08 (over 2) loglike:train/valid[45,69]=(-0.153,-0.116/-0.24,-0.23) accuracy:train/valid[45,69]=(0.941,0.9804/0.922,0.922)
steps/nnet3/train_raw_dnn.py --stage=-1 --cmd=slurm.pl --trainer.optimization.proportional-shrink 10 --trainer.optimization.momentum=0.5 --trainer.optimization.num-jobs-initial=3 --trainer.optimization.num-jobs-final=3 --trainer.optimization.initial-effective-lrate=0.001 --trainer.optimization.final-effective-lrate=0.0001 --trainer.optimization.minibatch-size=64 --trainer.srand=123 --trainer.max-param-change=2 --trainer.num-epochs=3 --trainer.dropout-schedule=0,0@0.20,0.1@0.50,0 --trainer.shuffle-buffer-size=1000 --egs.frames-per-eg=1 --egs.dir=/home/s1513472/lid/baseline/nnet/egs --cleanup.remove-egs false --cleanup.preserve-model-interval=10 --use-gpu=true --dir=/home/s1513472/lid/baseline/nnet
['steps/nnet3/train_raw_dnn.py', '--stage=-1', '--cmd=slurm.pl', '--trainer.optimization.proportional-shrink', '10', '--trainer.optimization.momentum=0.5', '--trainer.optimization.num-jobs-initial=3', '--trainer.optimization.num-jobs-final=3', '--trainer.optimization.initial-effective-lrate=0.001', '--trainer.optimization.final-effective-lrate=0.0001', '--trainer.optimization.minibatch-size=64', '--trainer.srand=123', '--trainer.max-param-change=2', '--trainer.num-epochs=3', '--trainer.dropout-schedule=0,0@0.20,0.1@0.50,0', '--trainer.shuffle-buffer-size=1000', '--egs.frames-per-eg=1', '--egs.dir=/home/s1513472/lid/baseline/nnet/egs', '--cleanup.remove-egs', 'false', '--cleanup.preserve-model-interval=10', '--use-gpu=true', '--dir=/home/s1513472/lid/baseline/nnet']
Obtaining renewable credentials, you will be asked to type in your password
Password for s1513472@INF.ED.AC.UK: 
Running job "./run.sh --experiment=baseline --stage=4" for maximum of 28 days in background
Waiting for job to start...
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 	 This shell script runs the GlobalPhone+X-vectors recipe.
 	 Use like this: ./run.sh <options>
 	 --stage=INT		Stage from which to start
 	 --run-all=(false|true)	Whether to run all stages
 	 			or just the specified one
 	 --experiment=STR	Experiment name (also name of directory 
 	 			where all files will be stored).
 	 			Default: 'baseline'.

 	 If no stage number is provided, either all stages
 	 will be run (--run-all=true) or no stages at all.
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Running only stage 4.
Conda environment not activated, sourcing ~/.bashrc and activating the 'lid' env.
Running on the cluster.
Using shorten (v3.6.1) from ~/language-ident-from-speech/gp-xvectors-recipe/tools/shorten-3.6.1/bin/shorten
Using sox (v14.3.2) from ~/language-ident-from-speech/gp-xvectors-recipe/tools/sox-14.3.2/bin/sox
Running with languages: AR BG CH CR CZ FR GE JA KO PL PO RU SP SW TA TH TU WU VN
#### STAGE 4: Training the X-vector DNN. ####
Running on the cluster.
Running on the cluster.
/home/s1513472/lid/baseline/nnet_train_data
/home/s1513472/lid/baseline/nnet/egs
#### STAGE 4: Getting NN training egs. ####
./local/get_egs.sh --cmd slurm.pl --nj 32 --stage 0 --frames-per-iter 50000000 --frames-per-iter-diagnostic 100000 --min-frames-per-chunk 200 --max-frames-per-chunk 400 --num-diagnostic-archives 3 --num-repeats 35 /home/s1513472/lid/baseline/nnet_train_data /home/s1513472/lid/baseline/nnet/egs
feat-to-dim scp:/home/s1513472/lid/baseline/nnet_train_data/feats.scp - 
./local/get_egs.sh: Preparing train and validation lists
./local/get_egs.sh: Producing 70 archives for training
./local/get_egs.sh: Allocating training examples
Job started successfully
./local/get_egs.sh: Allocating training subset examples
./local/get_egs.sh: Allocating validation examples
./local/get_egs.sh: Generating training examples on disk
./local/get_egs.sh: Generating training subset examples on disk
./local/get_egs.sh: Generating validation examples on disk
./local/get_egs.sh: Shuffling order of archives on disk
./local/get_egs.sh: Finished preparing training examples
#### STAGE 5: Creating NN configs using the xconfig parser. ####
./local/run_xvector.sh: creating neural net configs using the xconfig parser
nnet3-init /home/s1513472/lid/baseline/nnet/configs/ref.config /home/s1513472/lid/baseline/nnet/configs/ref.raw 
LOG (nnet3-init[5.5.142~5-ff514]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to /home/s1513472/lid/baseline/nnet/configs/ref.raw
nnet3-info /home/s1513472/lid/baseline/nnet/configs/ref.raw 
nnet3-init /home/s1513472/lid/baseline/nnet/configs/ref.config /home/s1513472/lid/baseline/nnet/configs/ref.raw 
LOG (nnet3-init[5.5.142~5-ff514]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to /home/s1513472/lid/baseline/nnet/configs/ref.raw
nnet3-info /home/s1513472/lid/baseline/nnet/configs/ref.raw 
steps/nnet3/xconfig_to_configs.py --xconfig-file /home/s1513472/lid/baseline/nnet/configs/network.xconfig --config-dir /home/s1513472/lid/baseline/nnet/configs
#### STAGE 6: Training the network. ####
2019-01-20 09:18:17,808 [steps/nnet3/train_raw_dnn.py:35 - <module> - INFO ] Starting raw DNN trainer (train_raw_dnn.py)
2019-01-20 09:18:18,121 [steps/nnet3/train_raw_dnn.py:156 - process_args - WARNING ] You are running with one thread but you have not compiled
                   for CUDA.  You may be running a setup optimized for GPUs.
                   If you have GPUs and have nvcc installed, go to src/ and do
                   ./configure; make
2019-01-20 09:18:18,123 [steps/nnet3/train_raw_dnn.py:195 - train - INFO ] Arguments for the experiment
{'backstitch_training_interval': 1,
 'backstitch_training_scale': 0.0,
 'cleanup': True,
 'cmvn_opts': None,
 'combine_sum_to_one_penalty': 0.0,
 'command': 'slurm.pl',
 'compute_average_posteriors': False,
 'compute_per_dim_accuracy': False,
 'dir': '/home/s1513472/lid/baseline/nnet',
 'do_final_combination': True,
 'dropout_schedule': '0,0@0.20,0.1@0.50,0',
 'egs_command': None,
 'egs_dir': '/home/s1513472/lid/baseline/nnet/egs',
 'egs_opts': None,
 'egs_stage': 0,
 'email': None,
 'exit_stage': None,
 'feat_dir': None,
 'final_effective_lrate': 0.0001,
 'frames_per_eg': 1,
 'image_augmentation_opts': None,
 'initial_effective_lrate': 0.001,
 'input_model': None,
 'max_lda_jobs': 10,
 'max_models_combine': 20,
 'max_objective_evaluations': 30,
 'max_param_change': 2.0,
 'minibatch_size': '64',
 'momentum': 0.5,
 'nj': 4,
 'num_epochs': 3.0,
 'num_jobs_compute_prior': 10,
 'num_jobs_final': 3,
 'num_jobs_initial': 3,
 'online_ivector_dir': None,
 'preserve_model_interval': 10,
 'presoftmax_prior_scale_power': -0.25,
 'prior_subset_size': 20000,
 'proportional_shrink': 10.0,
 'rand_prune': 4.0,
 'remove_egs': False,
 'reporting_interval': 0.1,
 'samples_per_iter': 400000,
 'shuffle_buffer_size': 1000,
 'srand': 123,
 'stage': -1,
 'targets_scp': None,
 'train_opts': [],
 'use_dense_targets': True,
 'use_gpu': 'yes'}
2019-01-20 09:18:18,156 [steps/nnet3/train_raw_dnn.py:315 - train - INFO ] Preparing the initial network.
2019-01-20 09:18:26,626 [steps/nnet3/train_raw_dnn.py:353 - train - INFO ] Training will run for 3.0 epochs = 70 iterations
2019-01-20 09:18:26,626 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 0/69    Epoch: 0.00/3.0 (0.0% complete)    lr: 0.003000    shrink: 0.97000
2019-01-20 09:25:57,400 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 1/69    Epoch: 0.04/3.0 (1.4% complete)    lr: 0.002903    shrink: 0.97097
2019-01-20 09:33:46,149 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 2/69    Epoch: 0.09/3.0 (2.9% complete)    lr: 0.002809    shrink: 0.97191
2019-01-20 09:38:58,629 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 3/69    Epoch: 0.13/3.0 (4.3% complete)    lr: 0.002718    shrink: 0.97282
2019-01-20 09:45:10,781 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 4/69    Epoch: 0.17/3.0 (5.7% complete)    lr: 0.002630    shrink: 0.97370
2019-01-20 09:52:47,829 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 5/69    Epoch: 0.21/3.0 (7.1% complete)    lr: 0.002545    shrink: 0.97455
2019-01-20 10:00:15,531 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 6/69    Epoch: 0.26/3.0 (8.6% complete)    lr: 0.002463    shrink: 0.97537
2019-01-20 10:07:58,751 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 7/69    Epoch: 0.30/3.0 (10.0% complete)    lr: 0.002383    shrink: 0.97617
2019-01-20 10:15:51,063 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 8/69    Epoch: 0.34/3.0 (11.4% complete)    lr: 0.002306    shrink: 0.97694
2019-01-20 10:21:32,926 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 9/69    Epoch: 0.39/3.0 (12.9% complete)    lr: 0.002231    shrink: 0.97769
2019-01-20 10:28:34,856 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 10/69    Epoch: 0.43/3.0 (14.3% complete)    lr: 0.002159    shrink: 0.97841
2019-01-20 10:34:20,988 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 11/69    Epoch: 0.47/3.0 (15.7% complete)    lr: 0.002089    shrink: 0.97911
2019-01-20 10:41:18,633 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 12/69    Epoch: 0.51/3.0 (17.1% complete)    lr: 0.002022    shrink: 0.97978
2019-01-20 10:48:28,304 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 13/69    Epoch: 0.56/3.0 (18.6% complete)    lr: 0.001956    shrink: 0.98044
2019-01-20 10:56:27,007 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 14/69    Epoch: 0.60/3.0 (20.0% complete)    lr: 0.001893    shrink: 0.98107
2019-01-20 11:03:46,241 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 15/69    Epoch: 0.64/3.0 (21.4% complete)    lr: 0.001832    shrink: 0.98168
2019-01-20 11:10:57,314 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 16/69    Epoch: 0.69/3.0 (22.9% complete)    lr: 0.001772    shrink: 0.98228
2019-01-20 11:19:01,881 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 17/69    Epoch: 0.73/3.0 (24.3% complete)    lr: 0.001715    shrink: 0.98285
2019-01-20 11:26:55,745 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 18/69    Epoch: 0.77/3.0 (25.7% complete)    lr: 0.001660    shrink: 0.98340
2019-01-20 11:34:28,131 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 19/69    Epoch: 0.81/3.0 (27.1% complete)    lr: 0.001606    shrink: 0.98394
2019-01-20 11:39:22,018 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 20/69    Epoch: 0.86/3.0 (28.6% complete)    lr: 0.001554    shrink: 0.98446
2019-01-20 11:46:11,430 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 21/69    Epoch: 0.90/3.0 (30.0% complete)    lr: 0.001504    shrink: 0.98496
2019-01-20 11:53:24,951 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 22/69    Epoch: 0.94/3.0 (31.4% complete)    lr: 0.001455    shrink: 0.98545
2019-01-20 12:01:08,455 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 23/69    Epoch: 0.99/3.0 (32.9% complete)    lr: 0.001408    shrink: 0.98592
2019-01-20 12:06:12,686 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 24/69    Epoch: 1.03/3.0 (34.3% complete)    lr: 0.001362    shrink: 0.98638
2019-01-20 12:14:06,372 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 25/69    Epoch: 1.07/3.0 (35.7% complete)    lr: 0.001318    shrink: 0.98682
2019-01-20 12:21:52,957 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 26/69    Epoch: 1.11/3.0 (37.1% complete)    lr: 0.001276    shrink: 0.98724
2019-01-20 12:27:35,339 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 27/69    Epoch: 1.16/3.0 (38.6% complete)    lr: 0.001234    shrink: 0.98766
2019-01-20 12:35:15,932 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 28/69    Epoch: 1.20/3.0 (40.0% complete)    lr: 0.001194    shrink: 0.98806
2019-01-20 12:42:32,563 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 29/69    Epoch: 1.24/3.0 (41.4% complete)    lr: 0.001156    shrink: 0.98844
2019-01-20 12:50:01,083 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 30/69    Epoch: 1.29/3.0 (42.9% complete)    lr: 0.001118    shrink: 0.98882
2019-01-20 12:57:52,079 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 31/69    Epoch: 1.33/3.0 (44.3% complete)    lr: 0.001082    shrink: 0.98918
2019-01-20 13:05:45,004 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 32/69    Epoch: 1.37/3.0 (45.7% complete)    lr: 0.001047    shrink: 0.98953
2019-01-20 13:12:47,119 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 33/69    Epoch: 1.41/3.0 (47.1% complete)    lr: 0.001013    shrink: 0.98987
2019-01-20 13:18:39,031 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 34/69    Epoch: 1.46/3.0 (48.6% complete)    lr: 0.000980    shrink: 0.99020
2019-01-20 13:25:38,747 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 35/69    Epoch: 1.50/3.0 (50.0% complete)    lr: 0.000949    shrink: 0.99051
2019-01-20 13:32:50,269 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 36/69    Epoch: 1.54/3.0 (51.4% complete)    lr: 0.000918    shrink: 0.99082
2019-01-20 13:40:46,601 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 37/69    Epoch: 1.59/3.0 (52.9% complete)    lr: 0.000888    shrink: 0.99112
2019-01-20 13:47:42,110 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 38/69    Epoch: 1.63/3.0 (54.3% complete)    lr: 0.000860    shrink: 0.99140
2019-01-20 13:55:02,123 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 39/69    Epoch: 1.67/3.0 (55.7% complete)    lr: 0.000832    shrink: 0.99168
2019-01-20 14:00:20,656 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 40/69    Epoch: 1.71/3.0 (57.1% complete)    lr: 0.000805    shrink: 0.99195
2019-01-20 14:08:29,162 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 41/69    Epoch: 1.76/3.0 (58.6% complete)    lr: 0.000779    shrink: 0.99221
2019-01-20 14:16:01,140 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 42/69    Epoch: 1.80/3.0 (60.0% complete)    lr: 0.000754    shrink: 0.99246
2019-01-20 14:23:33,334 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 43/69    Epoch: 1.84/3.0 (61.4% complete)    lr: 0.000729    shrink: 0.99271
2019-01-20 14:28:12,431 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 44/69    Epoch: 1.89/3.0 (62.9% complete)    lr: 0.000706    shrink: 0.99294
2019-01-20 14:35:22,873 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 45/69    Epoch: 1.93/3.0 (64.3% complete)    lr: 0.000683    shrink: 0.99317
2019-01-20 14:42:15,818 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 46/69    Epoch: 1.97/3.0 (65.7% complete)    lr: 0.000661    shrink: 0.99339
2019-01-20 14:50:03,327 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 47/69    Epoch: 2.01/3.0 (67.1% complete)    lr: 0.000639    shrink: 0.99361
2019-01-20 14:57:53,828 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 48/69    Epoch: 2.06/3.0 (68.6% complete)    lr: 0.000619    shrink: 0.99381
2019-01-20 15:05:41,337 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 49/69    Epoch: 2.10/3.0 (70.0% complete)    lr: 0.000599    shrink: 0.99401
2019-01-20 15:11:28,229 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 50/69    Epoch: 2.14/3.0 (71.4% complete)    lr: 0.000579    shrink: 0.99421
2019-01-20 15:18:22,359 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 51/69    Epoch: 2.19/3.0 (72.9% complete)    lr: 0.000560    shrink: 0.99440
2019-01-20 15:26:03,778 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 52/69    Epoch: 2.23/3.0 (74.3% complete)    lr: 0.000542    shrink: 0.99458
2019-01-20 15:33:33,078 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 53/69    Epoch: 2.27/3.0 (75.7% complete)    lr: 0.000525    shrink: 0.99475
2019-01-20 15:41:21,919 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 54/69    Epoch: 2.31/3.0 (77.1% complete)    lr: 0.000508    shrink: 0.99492
2019-01-20 15:49:15,531 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 55/69    Epoch: 2.36/3.0 (78.6% complete)    lr: 0.000491    shrink: 0.99509
2019-01-20 15:54:58,677 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 56/69    Epoch: 2.40/3.0 (80.0% complete)    lr: 0.000475    shrink: 0.99525
2019-01-20 16:02:02,135 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 57/69    Epoch: 2.44/3.0 (81.4% complete)    lr: 0.000460    shrink: 0.99540
2019-01-20 16:07:48,514 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 58/69    Epoch: 2.49/3.0 (82.9% complete)    lr: 0.000445    shrink: 0.99555
2019-01-20 16:15:00,242 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 59/69    Epoch: 2.53/3.0 (84.3% complete)    lr: 0.000431    shrink: 0.99569
2019-01-20 16:21:59,540 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 60/69    Epoch: 2.57/3.0 (85.7% complete)    lr: 0.000417    shrink: 0.99583
2019-01-20 16:30:00,750 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 61/69    Epoch: 2.61/3.0 (87.1% complete)    lr: 0.000403    shrink: 0.99597
2019-01-20 16:37:18,894 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 62/69    Epoch: 2.66/3.0 (88.6% complete)    lr: 0.000390    shrink: 0.99610
2019-01-20 16:43:17,766 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 63/69    Epoch: 2.70/3.0 (90.0% complete)    lr: 0.000378    shrink: 0.99622
2019-01-20 16:51:25,055 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 64/69    Epoch: 2.74/3.0 (91.4% complete)    lr: 0.000365    shrink: 0.99635
2019-01-20 16:59:22,440 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 65/69    Epoch: 2.79/3.0 (92.9% complete)    lr: 0.000354    shrink: 0.99646
2019-01-20 17:06:58,651 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 66/69    Epoch: 2.83/3.0 (94.3% complete)    lr: 0.000342    shrink: 0.99658
2019-01-20 17:11:53,696 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 67/69    Epoch: 2.87/3.0 (95.7% complete)    lr: 0.000331    shrink: 0.99669
2019-01-20 17:19:02,356 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 68/69    Epoch: 2.91/3.0 (97.1% complete)    lr: 0.000320    shrink: 0.99680
2019-01-20 17:25:53,976 [steps/nnet3/train_raw_dnn.py:388 - train - INFO ] Iter: 69/69    Epoch: 2.96/3.0 (98.6% complete)    lr: 0.000300    shrink: 0.99700
2019-01-20 17:33:36,243 [steps/nnet3/train_raw_dnn.py:440 - train - INFO ] Doing final combination to produce final.raw
2019-01-20 17:33:36,244 [steps/libs/nnet3/train/frame_level_objf/common.py:491 - combine_models - INFO ] Combining {51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70} models.
2019-01-20 17:37:47,756 [steps/nnet3/train_raw_dnn.py:462 - train - INFO ] Cleaning up the experiment directory /home/s1513472/lid/baseline/nnet
/home/s1513472/lid/baseline/nnet: num-iters=70 nj=3..3 num-params=4.5M dim=23->19 combine=-0.04->-0.03 (over 10) loglike:train/valid[45,69]=(-0.047,-0.038/-0.128,-0.168) accuracy:train/valid[45,69]=(1.0000,1.0000/0.9804,0.961)
steps/nnet3/train_raw_dnn.py --stage=-1 --cmd=slurm.pl --trainer.optimization.proportional-shrink 10 --trainer.optimization.momentum=0.5 --trainer.optimization.num-jobs-initial=3 --trainer.optimization.num-jobs-final=3 --trainer.optimization.initial-effective-lrate=0.001 --trainer.optimization.final-effective-lrate=0.0001 --trainer.optimization.minibatch-size=64 --trainer.srand=123 --trainer.max-param-change=2 --trainer.num-epochs=3 --trainer.dropout-schedule=0,0@0.20,0.1@0.50,0 --trainer.shuffle-buffer-size=1000 --egs.frames-per-eg=1 --egs.dir=/home/s1513472/lid/baseline/nnet/egs --cleanup.remove-egs false --cleanup.preserve-model-interval=10 --use-gpu=true --dir=/home/s1513472/lid/baseline/nnet
['steps/nnet3/train_raw_dnn.py', '--stage=-1', '--cmd=slurm.pl', '--trainer.optimization.proportional-shrink', '10', '--trainer.optimization.momentum=0.5', '--trainer.optimization.num-jobs-initial=3', '--trainer.optimization.num-jobs-final=3', '--trainer.optimization.initial-effective-lrate=0.001', '--trainer.optimization.final-effective-lrate=0.0001', '--trainer.optimization.minibatch-size=64', '--trainer.srand=123', '--trainer.max-param-change=2', '--trainer.num-epochs=3', '--trainer.dropout-schedule=0,0@0.20,0.1@0.50,0', '--trainer.shuffle-buffer-size=1000', '--egs.frames-per-eg=1', '--egs.dir=/home/s1513472/lid/baseline/nnet/egs', '--cleanup.remove-egs', 'false', '--cleanup.preserve-model-interval=10', '--use-gpu=true', '--dir=/home/s1513472/lid/baseline/nnet']
Obtaining renewable credentials, you will be asked to type in your password
Password for s1513472@INF.ED.AC.UK: 
Running job "./run.sh --experiment=baseline --stage=7" for maximum of 28 days in background
Waiting for job to start...
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 	 This shell script runs the GlobalPhone+X-vectors recipe.
 	 Use like this: ./run.sh <options>
 	 --stage=INT		Stage from which to start
 	 --run-all=(false|true)	Whether to run all stages
 	 			or just the specified one
 	 --experiment=STR	Experiment name (also name of directory 
 	 			where all files will be stored).
 	 			Default: 'baseline'.

 	 If no stage number is provided, either all stages
 	 will be run (--run-all=true) or no stages at all.
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Running only stage 7.
Conda environment not activated, sourcing ~/.bashrc and activating the 'lid' env.
Running on the cluster.
Using shorten (v3.6.1) from ~/language-ident-from-speech/gp-xvectors-recipe/tools/shorten-3.6.1/bin/shorten
Using sox (v14.3.2) from ~/language-ident-from-speech/gp-xvectors-recipe/tools/sox-14.3.2/bin/sox
Running with languages: AR BG CH CR CZ FR GE JA KO PL PO RU SP SW TA TH TU WU VN
#### STAGE 7: Extracting X-vectors from the trained DNN. ####
./local/extract_xvectors.sh --cmd slurm.pl --mem 6G --use-gpu true --nj 32 /home/s1513472/lid/baseline/nnet /home/s1513472/lid/baseline/enroll /home/s1513472/lid/baseline/exp/xvectors_enroll
./local/extract_xvectors.sh --cmd slurm.pl --mem 6G --use-gpu true --nj 32 /home/s1513472/lid/baseline/nnet /home/s1513472/lid/baseline/test /home/s1513472/lid/baseline/exp/xvectors_test
./local/extract_xvectors.sh --cmd slurm.pl --mem 6G --use-gpu true --nj 32 /home/s1513472/lid/baseline/nnet /home/s1513472/lid/baseline/eval /home/s1513472/lid/baseline/exp/xvectors_eval
No such file /home/s1513472/lid/baseline/eval/feats.scp
No such file /home/s1513472/lid/baseline/enroll/feats.scp
No such file /home/s1513472/lid/baseline/test/feats.scp
Done
cat: /tmp/lrj.pid.59486: No such file or directory
Usage: /bin/grep [OPTION]... PATTERN [FILE]...
Try '/bin/grep --help' for more information.
Job ran for less than 10 seconds. Problem?
/bin/rm: cannot remove '/tmp/lrj.pid.59486': No such file or directory
Obtaining renewable credentials, you will be asked to type in your password
Password for s1513472@INF.ED.AC.UK: 
Running job "./run.sh --experiment=baseline --stage=7" for maximum of 28 days in background
Waiting for job to start...
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 	 This shell script runs the GlobalPhone+X-vectors recipe.
 	 Use like this: ./run.sh <options>
 	 --stage=INT		Stage from which to start
 	 --run-all=(false|true)	Whether to run all stages
 	 			or just the specified one
 	 --experiment=STR	Experiment name (also name of directory 
 	 			where all files will be stored).
 	 			Default: 'baseline'.

 	 If no stage number is provided, either all stages
 	 will be run (--run-all=true) or no stages at all.
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Running only stage 7.
Conda environment not activated, sourcing ~/.bashrc and activating the 'lid' env.
Running on the cluster.
Using shorten (v3.6.1) from ~/language-ident-from-speech/gp-xvectors-recipe/tools/shorten-3.6.1/bin/shorten
Using sox (v14.3.2) from ~/language-ident-from-speech/gp-xvectors-recipe/tools/sox-14.3.2/bin/sox
Running with languages: AR BG CH CR CZ FR GE JA KO PL PO RU SP SW TA TH TU WU VN
#### STAGE 7: Extracting X-vectors from the trained DNN. ####
./local/extract_xvectors.sh --cmd slurm.pl --mem 6G --use-gpu true --nj 32 /home/s1513472/lid/baseline/nnet /home/s1513472/lid/baseline/enroll /home/s1513472/lid/baseline/exp/xvectors_enroll
./local/extract_xvectors.sh --cmd slurm.pl --mem 6G --use-gpu true --nj 32 /home/s1513472/lid/baseline/nnet /home/s1513472/lid/baseline/eval /home/s1513472/lid/baseline/exp/xvectors_eval
./local/extract_xvectors.sh --cmd slurm.pl --mem 6G --use-gpu true --nj 32 /home/s1513472/lid/baseline/nnet /home/s1513472/lid/baseline/test /home/s1513472/lid/baseline/exp/xvectors_test
./local/extract_xvectors.sh: using /home/s1513472/lid/baseline/nnet/extract.config to extract xvectors
./local/extract_xvectors.sh: using /home/s1513472/lid/baseline/nnet/extract.config to extract xvectors
./local/extract_xvectors.sh: using /home/s1513472/lid/baseline/nnet/extract.config to extract xvectors
./local/extract_xvectors.sh: extracting xvectors for /home/s1513472/lid/baseline/test
./local/extract_xvectors.sh: extracting xvectors from nnet
./local/extract_xvectors.sh: extracting xvectors for /home/s1513472/lid/baseline/enroll
./local/extract_xvectors.sh: extracting xvectors from nnet
./local/extract_xvectors.sh: extracting xvectors for /home/s1513472/lid/baseline/eval
./local/extract_xvectors.sh: extracting xvectors from nnet
Job started successfully
./local/extract_xvectors.sh: combining xvectors across jobs
./local/extract_xvectors.sh: computing mean of xvectors for each language
./local/extract_xvectors.sh: combining xvectors across jobs
./local/extract_xvectors.sh: computing mean of xvectors for each language
/home/s1513472/language-ident-from-speech/gp-xvectors-recipe/utils/slurm.pl: job failed with status 1, log is in /home/s1513472/lid/baseline/exp/xvectors_test/log/language_mean.log
/home/s1513472/language-ident-from-speech/gp-xvectors-recipe/utils/slurm.pl: job failed with status 1, log is in /home/s1513472/lid/baseline/exp/xvectors_enroll/log/language_mean.log
./local/extract_xvectors.sh: combining xvectors across jobs
./local/extract_xvectors.sh: computing mean of xvectors for each language
/home/s1513472/language-ident-from-speech/gp-xvectors-recipe/utils/slurm.pl: job failed with status 1, log is in /home/s1513472/lid/baseline/exp/xvectors_eval/log/language_mean.log
Done
Obtaining renewable credentials, you will be asked to type in your password
Password for s1513472@INF.ED.AC.UK: 
Running job "./run.sh --exp-config=conf/exp_configs/epochs2.conf --stage 7" for maximum of 28 days in background
Waiting for job to start...
Unknown argument: --stage, exiting
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 	 This shell script runs the GlobalPhone+X-vectors recipe.
 	 Use like this: ./run.sh <options>
 	 --stage=INT		Stage from which to start
 	 --run-all=(false|true)	Whether to run all stages
 	 			or just the specified one
 	 --experiment=STR	Experiment name (also name of directory 
 	 			where all files will be stored).
 	 			Default: 'baseline'.
 	 --exp-config=FILE	Config file with all kinds of options,
 	 			see conf/exp_default.conf for an example.
 	 			NOTE: Where arguments are passed on the command line,
 	 			the values overwrite those found in the config file.

 	 If no stage number is provided, either all stages
 	 will be run (--run-all=true) or no stages at all.
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
cat: /tmp/lrj.pid.192937: No such file or directory
Usage: /bin/grep [OPTION]... PATTERN [FILE]...
Try '/bin/grep --help' for more information.
Job ran for less than 10 seconds. Problem?
/bin/rm: cannot remove '/tmp/lrj.pid.192937': No such file or directory
